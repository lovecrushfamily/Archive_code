{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHXF26ZJpSaN",
        "outputId": "2822b7ad-376f-4aef-85fd-a75c7a796107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'machine-translation'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 49 (delta 8), reused 44 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (49/49), 6.37 MiB | 5.94 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "/content/machine-translation\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/quangster/machine-translation\n",
        "%cd machine-translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvMbLXwTp0eg",
        "outputId": "6ef81c3c-005d-468f-b478-ef9d99fa99fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Retrieving folder 1OEYSMb7DHvhpHDkyErFfepIjeyDOiDR4 dev\n",
            "Processing file 1Koyp92dplbh_S_9UW8wvskbzHW6Gb5Zw dev.en\n",
            "Processing file 1KVzIWM8IUIS_NdWpctOd_l3FIm901e6L dev.vi\n",
            "Retrieving folder 1FkG-m-LSXaXCrau3yD8s9_f8Llda3KoF test\n",
            "Processing file 18XurJYc9T8i4JKzGRknNIMzEBD5bLDex test.en\n",
            "Processing file 1atCidgee403dxm8mAWIXq9mlfdcYSXc_ test.vi\n",
            "Retrieving folder 1jrfK8TmZghXISDq7JI-LTZyItZRgnEn2 train\n",
            "Processing file 1jR128Bdo7vyQc1OPBCE6zEz0gXF6eUDY train.en\n",
            "Processing file 1hKt2ww1-zZHzXRPl_0ijxUxdWK57XKp1 train.vi\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Koyp92dplbh_S_9UW8wvskbzHW6Gb5Zw\n",
            "To: /content/machine-translation/data/dev/dev.en\n",
            "100% 1.42M/1.42M [00:00<00:00, 13.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KVzIWM8IUIS_NdWpctOd_l3FIm901e6L\n",
            "To: /content/machine-translation/data/dev/dev.vi\n",
            "100% 1.89M/1.89M [00:00<00:00, 16.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18XurJYc9T8i4JKzGRknNIMzEBD5bLDex\n",
            "To: /content/machine-translation/data/test/test.en\n",
            "100% 1.55M/1.55M [00:00<00:00, 11.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1atCidgee403dxm8mAWIXq9mlfdcYSXc_\n",
            "To: /content/machine-translation/data/test/test.vi\n",
            "100% 2.06M/2.06M [00:00<00:00, 17.3MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1jR128Bdo7vyQc1OPBCE6zEz0gXF6eUDY\n",
            "From (redirected): https://drive.google.com/uc?id=1jR128Bdo7vyQc1OPBCE6zEz0gXF6eUDY&confirm=t&uuid=0463fc9f-7af9-4af2-b6bb-b5aad72d495b\n",
            "To: /content/machine-translation/data/train/train.en\n",
            "100% 230M/230M [00:05<00:00, 42.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1hKt2ww1-zZHzXRPl_0ijxUxdWK57XKp1\n",
            "From (redirected): https://drive.google.com/uc?id=1hKt2ww1-zZHzXRPl_0ijxUxdWK57XKp1&confirm=t&uuid=ca9a47ab-9e82-453e-9971-18035644b77d\n",
            "To: /content/machine-translation/data/train/train.vi\n",
            "100% 302M/302M [00:02<00:00, 116MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gdown\n",
        "!gdown --folder 1cPdLNnTlsj3N1FE9x6_K608bCAaYaVGM -O data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PI9yfIjqF4_",
        "outputId": "3a42dfee-0a98-4ff4-fcd3-0417d4b038bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CcZL-rGwHAB",
        "outputId": "6e52102e-e6b9-4fe3-f841-991468da222a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from src.data import Vocabulary, EnTokenizer, ViTokenizer, MTDataset\n",
        "from src.utils.data import read_corpus\n",
        "\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_vocab = Vocabulary.load('./ckpts/en_vocab.json')\n",
        "vi_vocab = Vocabulary.load('./ckpts/vi_vocab.json')\n",
        "print(en_vocab)\n",
        "print(vi_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPIg6FqezVFJ",
        "outputId": "33f093fe-9cfe-465d-abaa-a790efe45979"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary[language=english, size=34687]\n",
            "Vocabulary[language=vietnamese, size=21681]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SPuppqg6wH3c"
      },
      "outputs": [],
      "source": [
        "def get_dataset():\n",
        "    vi_tokenizer = ViTokenizer()\n",
        "    en_tokenizer = EnTokenizer()\n",
        "\n",
        "    global en_vocab, vi_vocab\n",
        "\n",
        "    train_en_sents, train_vi_sents = read_corpus(\"./data\", \"train\")\n",
        "    train_en_sents = train_en_sents[:100000]\n",
        "    train_vi_sents = train_vi_sents[:100000]\n",
        "\n",
        "    train_en_sents = [en_tokenizer.tokenize(sent.lower()) for sent in tqdm(train_en_sents)]\n",
        "    train_vi_sents = [vi_tokenizer.tokenize(sent.lower()) for sent in tqdm(train_vi_sents)]\n",
        "\n",
        "    train_dataset = MTDataset(\n",
        "        inputs=[en_vocab.words2indexes(sent, add_sos_eos=True) for sent in train_en_sents],\n",
        "        outputs=[vi_vocab.words2indexes(sent, add_sos_eos=True) for sent in train_vi_sents],\n",
        "        max_length=20,\n",
        "        padding_idx=en_vocab['<pad>'],\n",
        "    )\n",
        "\n",
        "    val_en_sents, val_vi_sents = read_corpus(\"./data\", \"dev\")\n",
        "    val_en_sents = [en_tokenizer.tokenize(sent.lower()) for sent in tqdm(val_en_sents)]\n",
        "    val_vi_sents = [vi_tokenizer.tokenize(sent.lower()) for sent in tqdm(val_vi_sents)]\n",
        "\n",
        "    val_dataset = MTDataset(\n",
        "        inputs=[en_vocab.words2indexes(sent, add_sos_eos=True) for sent in val_en_sents],\n",
        "        outputs=[vi_vocab.words2indexes(sent, add_sos_eos=True) for sent in val_vi_sents],\n",
        "        max_length=20,\n",
        "        padding_idx=en_vocab['<pad>'],\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset = get_dataset()\n",
        "len(train_dataset), len(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmEotrVCy-FN",
        "outputId": "7a473ae5-489d-44fe-cebd-7d8670d56104"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [00:15<00:00, 6632.16it/s]\n",
            "100%|██████████| 100000/100000 [00:28<00:00, 3506.87it/s]\n",
            "100%|██████████| 18719/18719 [00:04<00:00, 4007.03it/s]\n",
            "100%|██████████| 18719/18719 [00:07<00:00, 2635.52it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46897, 11668)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# double check\n",
        "X, y = train_dataset[0]\n",
        "print(en_vocab.indexes2words(X.numpy()))\n",
        "print(vi_vocab.indexes2words(y.numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFpZ-mSQzNld",
        "outputId": "828d312b-9b8e-4231-91cc-d6ed2cdbb5d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', 'it', 'begins', 'with', 'a', 'countdown', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "['<sos>', 'câu', 'chuyện', 'bắt', 'đầu', 'với', 'buổi', 'lễ', 'đếm', 'ngược', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qoqWyjmwlU6"
      },
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RWTcxJ6jwc5X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "\n",
        "        assert hid_dim % n_heads == 0\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "\n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "\n",
        "        #x = [batch size, query len, hid dim]\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "\n",
        "        #x = [batch size, query len, hid dim]\n",
        "\n",
        "        return x, attention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x = [batch size, seq len, hid dim]\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "\n",
        "        #x = [batch size, seq len, pf dim]\n",
        "\n",
        "        x = self.fc_2(x)\n",
        "\n",
        "        #x = [batch size, seq len, hid dim]\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "rjpcZlEf1DPT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hid_dim,\n",
        "                 n_heads,\n",
        "                 pf_dim,\n",
        "                 dropout,\n",
        "                 device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n",
        "                                                                     pf_dim,\n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "\n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        #src = [batch size, src len, hid dim]\n",
        "\n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "\n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        #src = [batch size, src len, hid dim]\n",
        "\n",
        "        return src\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 hid_dim,\n",
        "                 n_layers,\n",
        "                 n_heads,\n",
        "                 pf_dim,\n",
        "                 dropout,\n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim,\n",
        "                                                  n_heads,\n",
        "                                                  pf_dim,\n",
        "                                                  dropout,\n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "        #pos = [batch size, src len]\n",
        "\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        #src = [batch size, src len, hid dim]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        #src = [batch size, src len, hid dim]\n",
        "\n",
        "        return src"
      ],
      "metadata": {
        "id": "Pb72I2LG1FqS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hid_dim,\n",
        "                 n_heads,\n",
        "                 pf_dim,\n",
        "                 dropout,\n",
        "                 device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n",
        "                                                                     pf_dim,\n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, enc_src, tgt_mask, src_mask):\n",
        "\n",
        "        #tgt = [batch size, tgt len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #tgt_mask = [batch size, 1, tgt len, tgt len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        #self attention\n",
        "        _tgt, _ = self.self_attention(tgt, tgt, tgt, tgt_mask)\n",
        "\n",
        "        #dropout, residual connection and layer norm\n",
        "        tgt = self.self_attn_layer_norm(tgt + self.dropout(_tgt))\n",
        "\n",
        "        #tgt = [batch size, tgt len, hid dim]\n",
        "\n",
        "        #encoder attention\n",
        "        _tgt, attention = self.encoder_attention(tgt, enc_src, enc_src, src_mask)\n",
        "\n",
        "        #dropout, residual connection and layer norm\n",
        "        tgt = self.enc_attn_layer_norm(tgt + self.dropout(_tgt))\n",
        "\n",
        "        #tgt = [batch size, tgt len, hid dim]\n",
        "\n",
        "        #positionwise feedforward\n",
        "        _tgt = self.positionwise_feedforward(tgt)\n",
        "\n",
        "        #dropout, residual and layer norm\n",
        "        tgt = self.ff_layer_norm(tgt + self.dropout(_tgt))\n",
        "\n",
        "        #tgt = [batch size, tgt len, hid dim]\n",
        "        #attention = [batch size, n heads, tgt len, src len]\n",
        "\n",
        "        return tgt, attention\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 output_dim,\n",
        "                 hid_dim,\n",
        "                 n_layers,\n",
        "                 n_heads,\n",
        "                 pf_dim,\n",
        "                 dropout,\n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim,\n",
        "                                                  n_heads,\n",
        "                                                  pf_dim,\n",
        "                                                  dropout,\n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "    def forward(self, tgt, enc_src, tgt_mask, src_mask):\n",
        "\n",
        "        #tgt = [batch size, tgt len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #tgt_mask = [batch size, 1, tgt len, tgt len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        batch_size = tgt.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, tgt_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "        #pos = [batch size, tgt len]\n",
        "\n",
        "        tgt = self.dropout((self.tok_embedding(tgt) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        #tgt = [batch size, tgt len, hid dim]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            tgt, attention = layer(tgt, enc_src, tgt_mask, src_mask)\n",
        "\n",
        "        #tgt = [batch size, tgt len, hid dim]\n",
        "        #attention = [batch size, n heads, tgt len, src len]\n",
        "\n",
        "        output = self.fc_out(tgt)\n",
        "\n",
        "        #output = [batch size, tgt len, output dim]\n",
        "\n",
        "        return output, attention"
      ],
      "metadata": {
        "id": "fFZpYBtD1IX7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 encoder,\n",
        "                 decoder,\n",
        "                 src_pad_idx,\n",
        "                 tgt_pad_idx,\n",
        "                 device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "\n",
        "        #src = [batch size, src len]\n",
        "\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "\n",
        "        #tgt = [batch size, tgt len]\n",
        "\n",
        "        tgt_pad_mask = (tgt != self.tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #tgt_pad_mask = [batch size, 1, 1, tgt len]\n",
        "\n",
        "        tgt_len = tgt.shape[1]\n",
        "\n",
        "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device = self.device)).bool()\n",
        "\n",
        "        #tgt_sub_mask = [tgt len, tgt len]\n",
        "\n",
        "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
        "\n",
        "        #tgt_mask = [batch size, 1, tgt len, tgt len]\n",
        "\n",
        "        return tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "\n",
        "        #src = [batch size, src len]\n",
        "        #tgt = [batch size, tgt len]\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #tgt_mask = [batch size, 1, tgt len, tgt len]\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "\n",
        "        output, attention = self.decoder(tgt, enc_src, tgt_mask, src_mask)\n",
        "\n",
        "        #output = [batch size, tgt len, output dim]\n",
        "        #attention = [batch size, n heads, tgt len, src len]\n",
        "\n",
        "        return output, attention"
      ],
      "metadata": {
        "id": "S3HZ1oZy1VvJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, \"weight\") and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "metadata": {
        "id": "MVTYCouk1Z3B"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, model, loader, optimizer, criterion, clip):\n",
        "    '''\n",
        "        Train step\n",
        "        Parameters:\n",
        "            epoch (int): the index of current epoch\n",
        "            model (Seq2Seq): trained NMT model\n",
        "            loader (DataLoader): train DataLoader\n",
        "            optimizer (torch.optim)\n",
        "            criterion (torch.nn)\n",
        "        Returns:\n",
        "            epoch_loss / len(loader) (float): average loss of this epoch\n",
        "    '''\n",
        "    model.train()\n",
        "    device = model.device\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with tqdm(enumerate(loader), total=len(loader)) as pbar:\n",
        "        pbar.set_description(f\"Epoch {epoch}\")\n",
        "        for i, (src, tgt) in pbar:\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output, _ = model(src, tgt[:,:-1])\n",
        "\n",
        "            #output = [batch size, tgt len - 1, output dim]\n",
        "            #tgt = [batch size, tgt len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            tgt = tgt[:,1:].contiguous().view(-1)\n",
        "\n",
        "            #output = [batch size * tgt len - 1, output dim]\n",
        "            #tgt = [batch size * tgt len - 1]\n",
        "\n",
        "            loss = criterion(output, tgt)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_postfix({\"loss\": epoch_loss/(i+1)})\n",
        "\n",
        "    return epoch_loss / len(loader)"
      ],
      "metadata": {
        "id": "FhfWK8_X1ga7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, criterion):\n",
        "    '''\n",
        "        Evaluate the model\n",
        "        Parameters:\n",
        "            model (Seq2Seq): trained NMT model\n",
        "            loader (DataLoader): test/valid DataLoader\n",
        "            criterion (torch.nn)\n",
        "        Returns:\n",
        "            epoch_loss / len(loader) (float): average loss\n",
        "    '''\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (src, tgt) in enumerate(loader):\n",
        "\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            output, _ = model(src, tgt[:,:-1])\n",
        "\n",
        "            #output = [batch size, tgt len - 1, output dim]\n",
        "            #tgt = [batch size, tgt len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            tgt = tgt[:,1:].contiguous().view(-1)\n",
        "\n",
        "            #output = [batch size * tgt len - 1, output dim]\n",
        "            #tgt = [batch size * tgt len - 1]\n",
        "\n",
        "            loss = criterion(output, tgt)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(loader)"
      ],
      "metadata": {
        "id": "CqgAkX1t2jDJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5UzhYJJtBrR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "enc_voc_size = len(en_vocab)\n",
        "dec_voc_size = len(vi_vocab)\n",
        "\n",
        "src_pad_id = en_vocab[\"<pad>\"]\n",
        "tgt_pad_id = vi_vocab[\"<pad>\"]\n",
        "\n",
        "d_model = 256\n",
        "max_len = 20\n",
        "ffn_hidden = 512\n",
        "n_heads = 8\n",
        "n_layers = 3\n",
        "drop_prob = 0.1\n",
        "\n",
        "clip = 1.0"
      ],
      "metadata": {
        "id": "g5AQXapo2zp8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = Encoder(input_dim=enc_voc_size,\n",
        "                    hid_dim=d_model,\n",
        "                    n_layers=n_layers,\n",
        "                    n_heads=n_heads,\n",
        "                    pf_dim=ffn_hidden,\n",
        "                    dropout=drop_prob,\n",
        "                    device=device,\n",
        "                    max_length=max_len)\n",
        "\n",
        "dec = Decoder(output_dim=dec_voc_size,\n",
        "                hid_dim=d_model,\n",
        "                n_layers=n_layers,\n",
        "                n_heads=n_heads,\n",
        "                pf_dim=ffn_hidden,\n",
        "                dropout=drop_prob,\n",
        "                device=device,\n",
        "                max_length=max_len)\n",
        "\n",
        "model = Seq2Seq(enc, dec, src_pad_id, tgt_pad_id, device).to(device)"
      ],
      "metadata": {
        "id": "BkoFX20O29T7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "print(len(train_loader), len(val_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHW4SALh4XWf",
        "outputId": "80e152db-8254-4f38-c42d-d0198792d4c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "367 92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_id)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(20):\n",
        "    train_loss = train(epoch, model, train_loader, optimizer, criterion, clip)\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    print(f\"Epoch: {epoch+1:02} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU64HOqH3pe0",
        "outputId": "9e50b1aa-d833-4593-f722-0fd052439abc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 367/367 [00:47<00:00,  7.71it/s, loss=4.49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Train Loss: 4.4871 | Val Loss: 4.3983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 367/367 [00:49<00:00,  7.47it/s, loss=3.49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Train Loss: 3.4855 | Val Loss: 4.0929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 367/367 [00:52<00:00,  7.01it/s, loss=3.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Train Loss: 3.1105 | Val Loss: 3.9345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 367/367 [00:56<00:00,  6.53it/s, loss=2.84]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Train Loss: 2.8443 | Val Loss: 3.8780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 367/367 [00:58<00:00,  6.24it/s, loss=2.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Train Loss: 2.6439 | Val Loss: 3.8291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 367/367 [00:51<00:00,  7.10it/s, loss=2.48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Train Loss: 2.4796 | Val Loss: 3.8359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 367/367 [00:53<00:00,  6.87it/s, loss=2.35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Train Loss: 2.3470 | Val Loss: 3.8245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 367/367 [01:00<00:00,  6.09it/s, loss=2.24]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Train Loss: 2.2418 | Val Loss: 3.8478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 367/367 [01:01<00:00,  6.01it/s, loss=2.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Train Loss: 2.1379 | Val Loss: 3.8690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 367/367 [00:49<00:00,  7.37it/s, loss=2.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Train Loss: 2.0604 | Val Loss: 3.9224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 367/367 [00:48<00:00,  7.51it/s, loss=1.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11 | Train Loss: 1.9917 | Val Loss: 3.8829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 367/367 [01:01<00:00,  5.96it/s, loss=1.93]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12 | Train Loss: 1.9273 | Val Loss: 3.9217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 367/367 [00:54<00:00,  6.71it/s, loss=1.87]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13 | Train Loss: 1.8661 | Val Loss: 3.9580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 367/367 [00:53<00:00,  6.83it/s, loss=1.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14 | Train Loss: 1.8160 | Val Loss: 3.9619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 367/367 [00:49<00:00,  7.48it/s, loss=1.77]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15 | Train Loss: 1.7706 | Val Loss: 4.0199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 367/367 [00:56<00:00,  6.44it/s, loss=1.73]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16 | Train Loss: 1.7296 | Val Loss: 4.0116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 367/367 [00:49<00:00,  7.40it/s, loss=1.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17 | Train Loss: 1.6850 | Val Loss: 4.0716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 367/367 [00:52<00:00,  7.06it/s, loss=1.65]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18 | Train Loss: 1.6526 | Val Loss: 4.1116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 367/367 [00:49<00:00,  7.41it/s, loss=1.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19 | Train Loss: 1.6168 | Val Loss: 4.1063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 367/367 [00:54<00:00,  6.78it/s, loss=1.59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20 | Train Loss: 1.5923 | Val Loss: 4.1184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sent, model, src_tok, tgt_tok, device, max_len=256):\n",
        "    '''\n",
        "        Translate sentence used trained model with autoregression decoding strategy\n",
        "        Parameters:\n",
        "            sent (str or list(str)): sentence string or list of tokens of\n",
        "                                     source language\n",
        "            src_tok (BaseTokenizer): tokenizer for source language\n",
        "            tgt_tok (BaseTokenizer): tokenizer for target language\n",
        "            model (Seq2Seq): trained NMT model\n",
        "            device (torch.device): cpu or cuda\n",
        "            max_len (int): maximum number of tokens for each sentence\n",
        "        Returns:\n",
        "            pred_tokens (list(str)): list of translated tokens\n",
        "            attention (Tensor): attention score from the last layer, can be used\n",
        "                                to create attention alignment matrix\n",
        "    '''\n",
        "    global en_vocab, vi_vocab\n",
        "\n",
        "    model.eval()\n",
        "    tokens = [token for token in src_tok.tokenize(sent.lower())]\n",
        "\n",
        "    tokens = en_vocab.words2indexes(tokens, add_sos_eos=True)\n",
        "    src_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    pred_indexes = [vi_vocab[\"<sos>\"]]\n",
        "\n",
        "    # Loop to predict next index from list of previous indexes until reach the\n",
        "    # end-of-string (eos) token or reach the maximum length\n",
        "    for i in range(max_len):\n",
        "        pred_tensor = torch.LongTensor(pred_indexes).unsqueeze(0).to(device)\n",
        "        tgt_mask = model.make_tgt_mask(pred_tensor)\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(pred_tensor, enc_src, tgt_mask, src_mask)\n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        pred_indexes.append(pred_token)\n",
        "        if pred_token == vi_vocab[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    pred_tokens = vi_vocab.indexes2words(pred_indexes)\n",
        "\n",
        "    return pred_tokens, attention"
      ],
      "metadata": {
        "id": "B6XMnp7l32rt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPgTmFq3_e5y",
        "outputId": "6f237b0e-2c82-4376-e857-7363e4050b3e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([    1,    14,  2278,    23,    11, 14547,     4,     2,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " tensor([   1,  248,  118,  155,   62,   21,  653,  934, 1742,  907,    4,    2,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vi_tok = ViTokenizer()\n",
        "en_tok = EnTokenizer()\n",
        "translate_sentence(\"Hello world\", model, vi_tok, en_tok, device, max_len=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RlUoy89-uQu",
        "outputId": "f705a333-69ec-4652-a333-52039a270a94"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<sos>', 'xin', 'chào', 'mừng', '!', '<eos>'],\n",
              " tensor([[[[0.0097, 0.8633, 0.1002, 0.0268],\n",
              "           [0.0554, 0.8052, 0.0872, 0.0523],\n",
              "           [0.0692, 0.4445, 0.3936, 0.0927],\n",
              "           [0.1086, 0.7410, 0.0925, 0.0578],\n",
              "           [0.2742, 0.0536, 0.5686, 0.1037]],\n",
              " \n",
              "          [[0.0137, 0.4405, 0.5328, 0.0130],\n",
              "           [0.0868, 0.6142, 0.1986, 0.1004],\n",
              "           [0.3024, 0.1860, 0.2952, 0.2163],\n",
              "           [0.2274, 0.3339, 0.3806, 0.0581],\n",
              "           [0.3492, 0.2872, 0.2873, 0.0763]],\n",
              " \n",
              "          [[0.0134, 0.7505, 0.1693, 0.0668],\n",
              "           [0.0277, 0.2613, 0.5625, 0.1485],\n",
              "           [0.1158, 0.1336, 0.3091, 0.4414],\n",
              "           [0.2343, 0.3869, 0.1282, 0.2506],\n",
              "           [0.4945, 0.0257, 0.3231, 0.1566]],\n",
              " \n",
              "          [[0.0073, 0.9510, 0.0344, 0.0073],\n",
              "           [0.0789, 0.8421, 0.0525, 0.0265],\n",
              "           [0.5312, 0.1537, 0.2035, 0.1117],\n",
              "           [0.7839, 0.0709, 0.0062, 0.1390],\n",
              "           [0.1664, 0.0677, 0.0318, 0.7342]],\n",
              " \n",
              "          [[0.0016, 0.9933, 0.0010, 0.0041],\n",
              "           [0.0143, 0.9223, 0.0374, 0.0261],\n",
              "           [0.0476, 0.4773, 0.1031, 0.3721],\n",
              "           [0.1255, 0.3121, 0.3828, 0.1796],\n",
              "           [0.4925, 0.2937, 0.0719, 0.1419]],\n",
              " \n",
              "          [[0.0222, 0.8091, 0.1430, 0.0257],\n",
              "           [0.3140, 0.4180, 0.0865, 0.1814],\n",
              "           [0.1116, 0.4729, 0.2867, 0.1288],\n",
              "           [0.0929, 0.0367, 0.8033, 0.0671],\n",
              "           [0.1943, 0.1114, 0.5779, 0.1163]],\n",
              " \n",
              "          [[0.0047, 0.9607, 0.0271, 0.0076],\n",
              "           [0.0594, 0.7346, 0.1264, 0.0795],\n",
              "           [0.3612, 0.2308, 0.2671, 0.1409],\n",
              "           [0.2533, 0.4078, 0.1338, 0.2051],\n",
              "           [0.4658, 0.0688, 0.2327, 0.2328]],\n",
              " \n",
              "          [[0.0103, 0.8508, 0.1237, 0.0152],\n",
              "           [0.0372, 0.7940, 0.0702, 0.0985],\n",
              "           [0.0890, 0.0397, 0.3995, 0.4719],\n",
              "           [0.3652, 0.1232, 0.2496, 0.2619],\n",
              "           [0.3973, 0.0521, 0.3846, 0.1660]]]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'transformer.pth')"
      ],
      "metadata": {
        "id": "UOjctJRg-9co"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_sentence(\"are you \", model, vi_tok, en_tok, device, max_len=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOTDJtdOA6nG",
        "outputId": "208bd522-6e02-4860-ba3c-39146442e4b5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<sos>', 'bạn', 'là', 'người', 'bạn', '.', '<eos>'],\n",
              " tensor([[[[2.4702e-02, 6.8987e-01, 2.1174e-01, 7.3688e-02],\n",
              "           [3.3886e-01, 1.6683e-01, 3.3255e-01, 1.6175e-01],\n",
              "           [7.1458e-01, 3.6809e-02, 2.2700e-01, 2.1616e-02],\n",
              "           [8.5895e-01, 8.9712e-03, 8.4375e-02, 4.7701e-02],\n",
              "           [9.2994e-01, 1.9175e-02, 1.8088e-02, 3.2792e-02],\n",
              "           [7.0587e-01, 1.6237e-01, 7.6674e-02, 5.5085e-02]],\n",
              " \n",
              "          [[9.4025e-02, 5.7760e-01, 2.4064e-01, 8.7742e-02],\n",
              "           [6.1075e-01, 1.7830e-01, 3.6227e-02, 1.7472e-01],\n",
              "           [7.6177e-01, 9.8738e-02, 6.2399e-02, 7.7092e-02],\n",
              "           [7.8850e-01, 1.3857e-02, 1.3848e-01, 5.9169e-02],\n",
              "           [9.6511e-01, 1.2167e-02, 7.9292e-03, 1.4798e-02],\n",
              "           [8.1791e-01, 6.9930e-02, 3.8902e-02, 7.3256e-02]],\n",
              " \n",
              "          [[2.6824e-02, 4.1240e-01, 4.4029e-01, 1.2049e-01],\n",
              "           [6.5002e-02, 8.3126e-01, 1.5240e-02, 8.8494e-02],\n",
              "           [3.8886e-01, 6.2735e-02, 4.0598e-01, 1.4243e-01],\n",
              "           [6.4960e-01, 4.4770e-02, 3.0624e-02, 2.7501e-01],\n",
              "           [9.2999e-01, 3.6438e-02, 9.1047e-03, 2.4468e-02],\n",
              "           [9.5016e-01, 6.7131e-03, 6.8760e-03, 3.6250e-02]],\n",
              " \n",
              "          [[1.4504e-01, 3.9138e-01, 3.1271e-01, 1.5086e-01],\n",
              "           [1.6369e-01, 6.0056e-01, 9.6752e-02, 1.3900e-01],\n",
              "           [3.9525e-01, 6.3970e-02, 7.5742e-02, 4.6504e-01],\n",
              "           [4.3557e-01, 1.7059e-02, 4.3656e-01, 1.1081e-01],\n",
              "           [1.6270e-01, 8.3915e-02, 9.3037e-02, 6.6035e-01],\n",
              "           [9.0877e-02, 2.3010e-02, 1.0806e-02, 8.7531e-01]],\n",
              " \n",
              "          [[2.9454e-02, 3.0147e-01, 5.4927e-01, 1.1981e-01],\n",
              "           [6.2635e-02, 3.8618e-01, 5.3280e-01, 1.8384e-02],\n",
              "           [7.0438e-01, 6.0557e-02, 1.9462e-01, 4.0435e-02],\n",
              "           [9.2348e-01, 2.5176e-02, 7.1806e-03, 4.4164e-02],\n",
              "           [9.7106e-01, 9.6666e-03, 1.8651e-02, 6.2575e-04],\n",
              "           [9.3994e-01, 1.7688e-02, 1.9344e-02, 2.3032e-02]],\n",
              " \n",
              "          [[4.3789e-02, 6.6580e-01, 2.4750e-01, 4.2909e-02],\n",
              "           [1.9811e-01, 3.6463e-01, 3.5403e-01, 8.3227e-02],\n",
              "           [3.7862e-01, 3.9988e-01, 5.1049e-02, 1.7045e-01],\n",
              "           [8.8038e-01, 3.3615e-02, 1.5103e-02, 7.0904e-02],\n",
              "           [8.0265e-01, 3.0696e-02, 3.6139e-02, 1.3052e-01],\n",
              "           [4.8240e-01, 2.5141e-01, 1.0784e-01, 1.5836e-01]],\n",
              " \n",
              "          [[3.2126e-02, 4.8622e-01, 4.2736e-01, 5.4294e-02],\n",
              "           [3.9858e-01, 2.4598e-01, 1.8076e-01, 1.7469e-01],\n",
              "           [4.2440e-01, 1.6565e-01, 2.7065e-01, 1.3930e-01],\n",
              "           [6.0122e-01, 1.1587e-02, 1.7241e-01, 2.1478e-01],\n",
              "           [9.1272e-01, 6.6052e-03, 1.3564e-02, 6.7111e-02],\n",
              "           [7.4893e-01, 6.2723e-02, 1.8805e-02, 1.6955e-01]],\n",
              " \n",
              "          [[5.3303e-02, 4.7049e-01, 3.7239e-01, 1.0382e-01],\n",
              "           [4.0930e-01, 2.6368e-02, 4.6432e-01, 1.0002e-01],\n",
              "           [5.5564e-01, 3.0458e-02, 2.8112e-01, 1.3278e-01],\n",
              "           [7.8561e-01, 4.4008e-02, 8.7964e-03, 1.6159e-01],\n",
              "           [9.7503e-01, 6.7712e-03, 1.5691e-02, 2.5082e-03],\n",
              "           [8.6442e-01, 1.0410e-01, 4.9589e-03, 2.6524e-02]]]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZrOqokiA7F9"
      },
      "execution_count": 25,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}